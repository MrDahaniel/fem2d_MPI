# Fem2d_MPI

El presente repositorio contiene todo lo relacionado con el desarrollo del proyecto final para las materias de Introducción A La Computación Paralela y Computación De Alto Rendimiento Y Científica del primer semestre de 2022 de la Universidad Industrial de Santander.

## 1. Identificando las oportunidades de paralelización

Como parte del proceso de paralelizar el código dado, lo primero a realizar está en identificar que secciones del código son las que más tiempo consumen. Para esto, usando el perfilador `gprof`, se ejecutó el código serial con `nx = 70` y `ny = 70`. Esto arrojó los siguientes resultados:

```
Flat profile:

Each sample counts as 0.01 seconds.
  %   cumulative   self              self     total
 time   seconds   seconds    calls   s/call   s/call  name
 99.38    491.63   491.63        1   491.63   491.63  r8ge_fs_new(int, double*, double*)
  0.62    494.71     3.08                             main
  0.00    494.71     0.00   180000     0.00     0.00  std::setw(int)
  0.00    494.71     0.00    23096     0.00     0.00  exact(double, double, double*, double*, double*)
  0.00    494.71     0.00        2     0.00     0.00  timestamp()
  0.00    494.71     0.00        1     0.00     0.00  __static_initialization_and_destruction_0(int, int)
```

De esta manera, se identifico que, en realidad, la tarea que más tiempo consumió era la función `r8ge_fs_new` la cual se encarga de resolver el sistema de ecuaciones. Siendo así se concentraron los esfuerzos en paralelizar esta sección del código.

## 2. Ejecutando en serial

```
FEM2D_POISSON_RECTANGLE_LINEAR
  C++ version

  Solution of the Poisson equation:

  - Uxx - Uyy = F(x,y) inside the region,
       U(x,y) = G(x,y) on the boundary of the region.

  The region is a rectangle, defined by:

  0 = XL<= X <= XR = 1
  0 = YB<= Y <= YT = 1

  The finite element method is used, with piecewise
  linear basis functions on 3 node triangular
  elements.

  The corner nodes of the triangles are generated by an
  underlying grid whose dimensions are

  NX =                       150
  NY =                       150
  Number of nodes =          22500
  Number of elements =       44402
...

FEM2D_POISSON_RECTANGLE_LINEAR:
  Normal end of execution.

./fem2d.out  431,31s user 0,69s system 99% cpu 7:13,29 total
```

## 3. Intentos de paralelizar

Enfoque en la solución de la matriz (Implementación parcial). Paralelizar generación de `x[]` y `y[]`.

```
...
NX =                       150
NY =                       150
Number of nodes =          22500
Number of elements =       44402

...

FEM2D_POISSON_RECTANGLE_LINEAR:
  Normal end of execution.

mpirun -np 8 fem2d_mpi.out  4039,59s user 6,06s system 797% cpu 8:27,16 total
```

En el caso de que se tenga una paralelización completa de la solución de la matriz, se esperarían los siguientes resultados:

```

...

FEM2D_POISSON_RECTANGLE_LINEAR:
  Normal end of execution.

mpirun -np 8 fem2d_mpi_bad.out  2561,84s user 6,70s system 797% cpu 5:21,98 total
```

En el mejor de los escenarios, se tendrían los siguientes resultados.

```
FEM2D_POISSON_RECTANGLE_LINEAR:
  Normal end of execution.

mpirun -np 8 fem2d_mpi.out  1810,16s user 7,64s system 794% cpu 3:48,73 total
```

## Ejecutando en Guane-1

Tenemos 2 opciones para correr nuestro código en Guane-1, de manera interactiva y de manera pasiva.

La primera requiere los siguientes pasos:

```console
$ git clone

$ cd code

$ module load devtools/mpi/openmpi/4.0.1

$ mpic++ -lm fem2d_poisson_mpi.cpp -o fem2d_mpi.out && time mpirun -np 8 fem2d_mpi.out

```

En el caso de querer correrlo de manera pasiva, lo primero es revisar las particiones

```
$ sinfo --all

PARTITION      AVAIL  TIMELIMIT  NODES  STATE NODELIST
normal*           up   infinite      1  down* guane04
normal*           up   infinite      2  drain guane[12,15]
normal*           up   infinite      2    mix guane[05,16]
normal*           up   infinite      7  alloc guane[01,03,06,09-10,13-14]
normal*           up   infinite      2   idle guane[07-08]
normal*           up   infinite      1   down guane11
guane_16_cores    up   infinite      1    mix guane05
guane_16_cores    up   infinite      2  alloc guane[03,06]
guane_16_CPU      up   infinite      2   idle guane[07-08]
guane_24_cores    up   infinite      1  down* guane04
guane_24_cores    up   infinite      2  drain guane[12,15]
guane_24_cores    up   infinite      1    mix guane16
guane_24_cores    up   infinite      5  alloc guane[01,09-10,13-14]
guane_24_cores    up   infinite      1   down guane11
Viz               up   infinite      1   idle yaje
deepL             up   infinite      1  alloc felix
```

Idealmente escogemos uno que esté en `idle`, y usamos el siguiente script:

```
#!/bin/bash
#SBATCH --partition=guane_16_CPU
#SBATCH -o mpi.%j.out       #Nombre del archivo de salida
#SBATCH -J fem2d_MPI_job    #Nombre del trabajo
#SBATCH --nodes=1           #Numero de nodos para correr el trabajo
#SBATCH --ntasks=10         #Numero de procesos
#SBATCH --tasks-per-node=10   #Numero de trabajos por nodo

#Prepara el ambiente de trabajo
export I_MPI_PMI_LIBRARY=module load devtools/mpi/openmpi/4.0.1
ulimit -l unlimited
export OMPI_MCA_btl=^openib

#Ejecuta el programa paralelo
srun ./fem2d_poisson_mpi.cpp

```

## Desarrollo Por

-   Didier Fernando Vallejo Sanabria - [yiye77](https://github.com/yiye77)
-   Sebastian Camilo Viancha Bautista - [SCBViancha](https://github.com/SCBViancha)
-   Yelitza Juliana Villamizar Guerrero - [Ywashere](https://github.com/Ywashere)
-   Daniel David Delgado Cervantes - [mrdahaniel](https://github.com/mrdahaniel)
-   Laura Daniela Medina Paipilla - [lauradanielamedina](https://github.com/lauradanielamedina)
-   Andrea Juliana Urrego Paredes - [Juliana18p](https://github.com/Juliana18p)
